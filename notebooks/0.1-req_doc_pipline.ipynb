{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "1316 rows in training set\n",
      "Found 3810 unique words in texts\n",
      "Loaded 94 stop words from /home/od13/addons/tender_cat/data/model/trained/1/2020_stop_word.txt\n",
      "Loaded 189 stop words from /home/od13/addons/tender_cat/data/model/trained/1/stop_words.mod\n",
      "Now 189 unique words in stop-word list\n",
      "Stop words cleaned from texts\n",
      "Deleted 0 texts containing stop-words only\n",
      "1316 rows in training set\n",
      "1316 rows in training set\n",
      "0 attention words marked\n",
      "719 words replaced with nice form\n",
      "--------------------------------------\n",
      "best_params:\n",
      "{'deep_sim__epochs': 500, 'deep_sim__sim_coef': 3, 'deep_sim__stack_size': 1}\n",
      "--------------------------------------\n",
      "best_estimator:\n",
      "Pipeline(steps=[('clean_text', CleanText()), ('clean_punct', CleanPunct()),\n",
      "                ('stop_word',\n",
      "                 StopWord(save_frequency_to_csv=None,\n",
      "                          trained_folder='/home/od13/addons/tender_cat/data/model/trained/1')),\n",
      "                ('clean_numbered_lists', CleanNumberedLists()),\n",
      "                ('attention_words', AttentionWord(folder=None)),\n",
      "                ('stemmer', StemWords()),\n",
      "                ('nice_stemmer', NiceStemWords(folder=None)),\n",
      "                ('ngram_database',\n",
      "                 NGramDB(trained_folder='/home/od13/addons/tender_cat/data/model/trained/1')),\n",
      "                ('text_registry_database',\n",
      "                 TextRegistryDB(trained_folder='/home/od13/addons/tender_cat/data/model/trained/1')),\n",
      "                ('deep_sim',\n",
      "                 DeepSimProd(epochs=500, stack_size=1,\n",
      "                             trained_folder='/home/od13/addons/tender_cat/data/model/trained/1'))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/od13/venv/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   20.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   37.5s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   43.4s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 101 out of 105 | elapsed:  5.4min remaining:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done 105 out of 105 | elapsed:  5.5min finished\n",
      "/home/od13/venv/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv, os, joblib, re, math, logging, gensim\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from uk_stemmer import UkStemmer\n",
    "from gensim.models import Phrases\n",
    "from gensim.parsing import preprocessing\n",
    "from pprint import pprint\n",
    "from pyhash import murmur3_x64_128\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "_set_name = 'training set'\n",
    "_no_label_id = 1001\n",
    "_no_label_name = '?'\n",
    "\n",
    "_debug_csv = '/home/od13/addons/tender_cat/data/model/debug0.csv'\n",
    "_test_set_csv = '/home/od13/addons/tender_cat/data/model/test_set.csv'\n",
    "\n",
    "#_data_folder='/home/od13/addons/tender_cat/data/model/dump/1'\n",
    "_data_folder='/home/od13/addons/tender_cat/data0/model/dump/1'\n",
    "_model_folder='/home/od13/addons/tender_cat/data/model/trained/1'\n",
    "\n",
    "def log(msg):\n",
    "    if len(str(msg)) > 255:\n",
    "        pprint(msg)\n",
    "    else:\n",
    "        print(msg)\n",
    "    _logger.info(msg)\n",
    "\n",
    "def print_sorted(source_df, column, topn=10, msg='', text_column='text', ascending=False):\n",
    "    log('\\nTop {} {}: '.format(topn, msg))\n",
    "    sort_df = pd.concat([source_df[column],source_df[text_column]], axis=1)\n",
    "    sort_df.apply(lambda row: int(row[column]), axis=1)\n",
    "    sort_df.drop_duplicates(subset=text_column, inplace=True)\n",
    "    top = topn\n",
    "    for i, row in sort_df.sort_values(column, ascending=ascending).iterrows():\n",
    "        log('{} {}'.format(row[column], row[text_column]))\n",
    "        top -= 1\n",
    "        if not top:\n",
    "            break\n",
    "\n",
    "def hash_str(hasher, string):\n",
    "    return str(hasher(str(string).strip()))\n",
    "\n",
    "def print_most_freq(source_df, column, top, msg='', reverse=True):\n",
    "        log('\\nTop {} {}: '.format(top, msg))\n",
    "        freq_dct = dict(source_df[column].value_counts())\n",
    "        for w in sorted(freq_dct, key=freq_dct.get, reverse=reverse):\n",
    "            print(freq_dct[w], w)\n",
    "            top -= 1\n",
    "            if not top:\n",
    "                break\n",
    "\n",
    "def get_subfolder(root, sub):\n",
    "    fdir = os.path.join(root, sub)\n",
    "    if not os.path.isdir(fdir):\n",
    "        os.makedirs(fdir)\n",
    "    return fdir\n",
    "\n",
    "def folder_csv_to_dataframe(folder):\n",
    "    csvs = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            if f.endswith(\".csv\"):\n",
    "                try:\n",
    "                    csvs.append(pd.read_csv(os.path.join(folder, f)))\n",
    "                except (FileExistsError, IOError, pd.errors.EmptyDataError) as e:\n",
    "                    _logger.error('{}: {}'.format(f, e))\n",
    "    if csvs:\n",
    "        return pd.concat(csvs, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def log_size(X, msg=None):\n",
    "    log_msg = '{} rows in {}'.format(X.shape[0], _set_name)\n",
    "    if msg is not None:\n",
    "        log_msg += ', {}'.format(msg)\n",
    "    log(log_msg)\n",
    "\n",
    "def word_src(source_df, column):\n",
    "    for i, row in source_df.iterrows():\n",
    "        txt = str(row[column]).strip()\n",
    "        if txt:\n",
    "            yield txt.split()\n",
    "\n",
    "def drop_epmty(X, column, msg=''):\n",
    "        start_len = X.shape[0]\n",
    "        #X = X.drop(X[X[column] == ''].index)\n",
    "        #log('Deleted {} rows {}'.format(start_len - X.shape[0], msg))\n",
    "        return X\n",
    "\n",
    "class PreprocessDF(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #log('--- PreprocessDF transform')\n",
    "        # X.fillna(inplace=True, value={'label_id': self.no_label_id,'label_name': self.no_label_name})\n",
    "        # X.label_id = X.label_id.astype(int)\n",
    "        log_size(X, 'preprocessed')\n",
    "        return X\n",
    "\n",
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def clean_txt(self, origin_txt):\n",
    "\n",
    "        txt = origin_txt.strip()\n",
    "\n",
    "        url_pattern = r'https?://\\S+|http?://\\S+|www\\.\\S+'\n",
    "        txt = re.sub(pattern=url_pattern, repl=' ', string=txt)\n",
    "\n",
    "        #number_pattern = r'\\d+'\n",
    "        #txt = re.sub(pattern=number_pattern, repl=\"nmbr\", string=txt)\n",
    "        #txt = re.sub(pattern=number_pattern, repl=\" \", string=txt)\n",
    "\n",
    "        single_char_pattern = r'\\s+[a-zA-Z]\\s+'\n",
    "        txt = re.sub(pattern=single_char_pattern, repl=\" \", string=txt)\n",
    "\n",
    "        txt = gensim.utils.decode_htmlentities(gensim.utils.deaccent(txt))\n",
    "\n",
    "        space_pattern = r'\\s+'\n",
    "        txt = re.sub(pattern=space_pattern, repl=\" \", string=txt)\n",
    "\n",
    "        # All characters are non alfa or latin characters\n",
    "        # or abbreviations\n",
    "        if re.match(\"^[\\d .«»,-:+*_;%@\\\\/(A-Z)(\\d*\\s*х\\d*\\s*)(С|м|шт|кг|т|СН)]+$\",\n",
    "                    txt, flags=re.IGNORECASE):\n",
    "            txt = ''\n",
    "\n",
    "        # Emails and email captions\n",
    "        pattern = r'(email|e-mail)\\:+'\n",
    "        txt = re.sub(pattern=pattern, repl=\"\", string=txt, flags=re.IGNORECASE)\n",
    "\n",
    "        pattern = r'([\\S*|\\d*])*@([\\S*|\\d*])*(\\s?)'\n",
    "        txt = re.sub(pattern=pattern, repl=\"\", string=txt, flags=re.IGNORECASE)\n",
    "\n",
    "        # Phone captions\n",
    "        pattern = r'(тел.|тел.)[\\:|\\s*]+'\n",
    "        txt = re.sub(pattern=pattern, repl=\"\", string=txt, flags=re.IGNORECASE)\n",
    "        pattern = r'(факс)[\\:|\\s*]+'\n",
    "        txt = re.sub(pattern=pattern, repl=\"\", string=txt, flags=re.IGNORECASE)\n",
    "\n",
    "        # Phone numbers\n",
    "        # (095) 354-25-25\n",
    "        pattern = r'(\\(\\d{1,7}\\)\\s*\\d{1,5}-\\d{1,5}-\\d{1,5})'\n",
    "        txt = re.sub(pattern=pattern, repl=\"\", string=txt, flags=re.IGNORECASE)\n",
    "        # 06264–2-01-35\n",
    "        pattern = r'\\s*(\\d{1,7}–\\d{1,5}–\\d{1,5}–\\d{1,5})'\n",
    "        txt = re.sub(pattern=pattern, repl=\"\", string=txt, flags=re.IGNORECASE)\n",
    "        # +380993004541\n",
    "        pattern = r'\\+(\\d{5,15})'\n",
    "        txt = re.sub(pattern=pattern, repl=\"\", string=txt, flags=re.IGNORECASE)\n",
    "\n",
    "        # М.П.\n",
    "        pattern = r'(м.п.)[^\\w]*'\n",
    "        txt = re.sub(pattern=pattern, repl=\"\", string=txt, flags=re.IGNORECASE)\n",
    "\n",
    "        txt = txt.strip()\n",
    "\n",
    "        return txt\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #log('--- CleanText transform')\n",
    "\n",
    "        X['clean_text'] = X['text'].apply(lambda x: self.clean_txt(x).lower())\n",
    "\n",
    "        X = drop_epmty(X, 'clean_text', 'after cleaning')\n",
    "        log_size(X)\n",
    "        #X.to_csv(_debug_csv, columns=['clean_text', 'text'])\n",
    "        return X\n",
    "\n",
    "class StopText(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, folder, save_frequency_to_csv=False):\n",
    "        self.frequency_to_csv = save_frequency_to_csv\n",
    "        self.stop_text = []\n",
    "        self.data_folder = folder\n",
    "        self.stat = {'chr_max': 0, 'chr_min': 0, 'chr_mean': 0 }\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Read files named any-prefix-stop_text.txt from data folder, load their content as stop-texts\n",
    "        \"\"\"\n",
    "        fdir = os.path.join(self.data_folder)\n",
    "        for f in os.listdir(fdir):\n",
    "            if os.path.isfile(os.path.join(fdir, f)):\n",
    "                if f.endswith(\"stop_text.txt\"):\n",
    "                    fname = os.path.join(fdir, f)\n",
    "                    with open(fname, \"r\") as file:\n",
    "                        cnt = 0\n",
    "                        for line in file:\n",
    "                            line_lst = list(line.strip().split(\" \"))\n",
    "                            txt = ' '.join(line_lst[1:])\n",
    "                            self.stop_text.append(txt)\n",
    "                            cnt += 1\n",
    "                        log('Loaded {} stop texts from {}'.format(cnt, fname))\n",
    "        self.stop_text = list(set(self.stop_text))\n",
    "\n",
    "        # Count characters\n",
    "        X['chr_count'] = X.apply(lambda row: len(row['clean_text']), axis=1)\n",
    "        stat = {\n",
    "            'chr_max': X['chr_count'].max(),\n",
    "            'chr_min': X['chr_count'].min(),\n",
    "            'chr_mean': X['chr_count'].mean()\n",
    "        }\n",
    "\n",
    "        log('Character counts in texts: min {}, max {},  mean {}'\n",
    "            .format(stat['chr_min'], stat['chr_max'], stat['chr_mean']))\n",
    "        self.stat.update(stat)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Delete stop-texts from X. Write text frequency to text file before this\n",
    "        (if enabled by arg save_frequency_to_csv)\n",
    "        \"\"\"\n",
    "        #log('--- StopText transform')\n",
    "        if self.frequency_to_csv:\n",
    "            # Save sorted text frequency to text file\n",
    "            fdir = os.path.join(self.data_folder, 'stop_text')\n",
    "            if not os.path.isdir(fdir):\n",
    "                os.makedirs(fdir)\n",
    "            fname = os.path.join(get_subfolder(self.data_folder, 'stop_text'),\n",
    "                                 datetime.today().strftime('%Y_%m_%d_%H_%M_%S')+'_stop_text.txt')\n",
    "            with open(fname, 'w') as f:\n",
    "                freq_dct = dict(X['clean_text'].value_counts())\n",
    "                text = ''\n",
    "                for w in sorted(freq_dct, key=freq_dct.get, reverse=True):\n",
    "                    text += '{} {}\\n'.format(freq_dct[w], w)\n",
    "                f.write(text)\n",
    "                log('Saved to {} {} texts'.format(fname, len(freq_dct)))\n",
    "\n",
    "        start_len = X.shape[0]\n",
    "        #X = X.drop(X[X['clean_text'].isin(self.stop_text)].index)\n",
    "        X.loc[X['clean_text'].isin(self.stop_text), 'clean_text'] = ''\n",
    "        log('{} rows containing stop-text deleted from training set'.format(start_len - X.shape[0]))\n",
    "        log_size(X)\n",
    "\n",
    "        # Delete too long and too short texts\n",
    "        stat = self.stat\n",
    "        #print_sorted(X,'chr_count',topn=100,ascending=True)\n",
    "        start_len = X.shape[0]\n",
    "        #X = X.drop(X[X.chr_count >= stat['chr_max'] ].index)\n",
    "        X.loc[X.chr_count >= stat['chr_max'], 'clean_text'] = ''\n",
    "        log('Deleted {} long texts, {} character length'.format(start_len - X.shape[0], stat['chr_max']))\n",
    "\n",
    "        start_len = X.shape[0]\n",
    "        #X = X.drop(X[X.chr_count <= stat['chr_min']+1].index)\n",
    "        X.loc[X.chr_count <= stat['chr_min']+1, 'clean_text'] = ''\n",
    "        log('Deleted {} short texts, {} and less characters length'.format(start_len - X.shape[0], stat['chr_min']+1))\n",
    "        log_size(X)\n",
    "        return X\n",
    "\n",
    "class StopWordTrain(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def is_dirty_word(self, word):\n",
    "        # All characters are non alfa or latin characters\n",
    "        # or abbreviations\n",
    "        if re.match('^[\\d  .,-«»:+*_;%@\\\\/(A-Z)(\\d*\\s*х\\d*\\s*)№(С|м|шт|кг|т|СН)]+$',\n",
    "                    word,\n",
    "                    flags=re.IGNORECASE):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def __init__(self, folder, save_frequency_to_csv=False):\n",
    "        self.frequency_to_csv = save_frequency_to_csv\n",
    "        self.stop_words = []\n",
    "        self.trained_folder = folder\n",
    "        self.stat = {'min': 0, 'max': 0, 'mean': 0 }\n",
    "        self.word_df = pd.DataFrame(columns=['word'])\n",
    "        self.file_name = os.path.join(self.trained_folder, 'stop_words.mod')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        all_words = []\n",
    "        for word_lst in word_src(X,'clean_text'):\n",
    "            for word in word_lst:\n",
    "                all_words.append(word)\n",
    "        self.word_df = pd.DataFrame(all_words, columns=['word'])\n",
    "        log('Found {} unique words in texts'.format(self.word_df['word'].nunique()))\n",
    "\n",
    "        freq_word_dct = dict(self.word_df['word'].value_counts())\n",
    "        cnt = 0\n",
    "        for w in freq_word_dct:\n",
    "             if freq_word_dct[w] == 1:\n",
    "                 if self.is_dirty_word(w):\n",
    "                    self.stop_words.append(w)\n",
    "                    cnt += 1\n",
    "        log('{} dirty rare words added to stop list'.format(cnt))\n",
    "\n",
    "        fdir = os.path.join(self.trained_folder)\n",
    "        for f in os.listdir(fdir):\n",
    "            if os.path.isfile(os.path.join(fdir, f)):\n",
    "                if f.endswith(\"stop_word.txt\"):\n",
    "                    fname = os.path.join(fdir, f)\n",
    "                    with open(fname, \"r\") as file:\n",
    "                        cnt = 0\n",
    "                        for line in file:\n",
    "                            line_lst = list(line.strip().split(\" \"))\n",
    "                            txt = ' '.join(line_lst[1:])\n",
    "                            self.stop_words.append(txt)\n",
    "                            cnt += 1\n",
    "                        log('Loaded {} stop words from {}'.format(cnt, fname))\n",
    "        self.stop_words = list(set(self.stop_words))\n",
    "\n",
    "        self.word_df = self.word_df.drop(self.word_df[self.word_df.word.isin(self.stop_words)].index)\n",
    "        #print_most_freq(self.word_df, 'word', 200, msg='frequent words')\n",
    "        #print_most_freq(self.word_df, 'word', 1000, msg='rare words', reverse=False)\n",
    "\n",
    "        with open(self.file_name, 'w') as f:\n",
    "            text = ''\n",
    "            for w in self.stop_words:\n",
    "                text += '{}\\n'.format(w)\n",
    "            f.write(text)\n",
    "            log('Stop words saved to {}'.format(self.file_name))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #log('--- StopWord transform')\n",
    "        if self.frequency_to_csv:\n",
    "            # Save sorted word frequency to text file\n",
    "            fname = os.path.join(get_subfolder(self.file_name, 'stop_word'),\n",
    "                                 datetime.today().strftime('%Y_%m_%d_%H_%M_%S')+'_stop_word.txt')\n",
    "            with open(fname, 'w') as f:\n",
    "                freq_dct = dict(self.word_df['word'].value_counts())\n",
    "                text = ''\n",
    "                for w in sorted(freq_dct, key=freq_dct.get, reverse=True):\n",
    "                    text += '{} {}\\n'.format(freq_dct[w], w)\n",
    "                f.write(text)\n",
    "                log('Saved {} words to {}'.format(len(freq_dct), fname))\n",
    "\n",
    "        if os.path.isfile(self.file_name):\n",
    "            with open(self.file_name, \"r\") as file:\n",
    "                cnt = 0\n",
    "                for line in file:\n",
    "                    self.stop_words.append(line.strip())\n",
    "                    cnt += 1\n",
    "                log('Loaded {} stop words from {}'.format(cnt, self.file_name))\n",
    "\n",
    "        X['clean_text'] = X['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if w.lower() not in self.stop_words]).strip())\n",
    "        log('Stop words cleaned from texts')\n",
    "        start_len = X.shape[0]\n",
    "        #X = X.drop(X[X.clean_text.isin(self.stop_words)].index)\n",
    "        X.loc[X.clean_text.isin(self.stop_words), 'clean_text'] = ''\n",
    "        # X = X.drop(X[X.clean_text == ''].index)\n",
    "        log('Deleted {} texts containing stop-words only'.format(start_len - X.shape[0]))\n",
    "        log_size(X)\n",
    "        #X.to_csv(_debug_csv, columns=['clean_text', 'text'])\n",
    "\n",
    "        return X\n",
    "\n",
    "class StopWord(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, trained_folder='', save_frequency_to_csv=False):\n",
    "        self.frequency_to_csv = save_frequency_to_csv\n",
    "        self.stop_words = []\n",
    "        self.trained_folder = trained_folder\n",
    "        self.file_name = os.path.join(trained_folder, 'stop_words.mod')\n",
    "        self.word_df = pd.DataFrame(columns=['word'])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        all_words = []\n",
    "        for word_lst in word_src(X,'clean_text'):\n",
    "            for word in word_lst:\n",
    "                all_words.append(word)\n",
    "        self.word_df = pd.DataFrame(all_words, columns=['word'])\n",
    "        log('Found {} unique words in texts'.format(self.word_df['word'].nunique()))\n",
    "\n",
    "        self.stop_words = []\n",
    "\n",
    "        fdir = os.path.join(self.trained_folder)\n",
    "        for f in os.listdir(fdir):\n",
    "            if os.path.isfile(os.path.join(fdir, f)):\n",
    "                if f.endswith(\"stop_word.txt\"):\n",
    "                    fname = os.path.join(fdir, f)\n",
    "                    with open(fname, \"r\") as file:\n",
    "                        cnt = 0\n",
    "                        for line in file:\n",
    "                            line_lst = list(line.strip().split(\" \"))\n",
    "                            txt = ' '.join(line_lst[1:])\n",
    "                            self.stop_words.append(txt)\n",
    "                            cnt += 1\n",
    "                        log('Loaded {} stop words from {}'.format(cnt, fname))\n",
    "        if os.path.isfile(self.file_name):\n",
    "            with open(self.file_name, \"r\") as file:\n",
    "                cnt = 0\n",
    "                for line in file:\n",
    "                    self.stop_words.append(line.strip())\n",
    "                    cnt += 1\n",
    "                log('Loaded {} stop words from {}'.format(cnt, self.file_name))\n",
    "\n",
    "        self.stop_words = list(set(self.stop_words))\n",
    "        log('Now {} unique words in stop-word list'.format(len(self.stop_words)))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #log('--- StopWord transform')\n",
    "        if self.frequency_to_csv:\n",
    "            # Save sorted word frequency to text file\n",
    "            fname = os.path.join(get_subfolder(self.file_name, 'stop_word'),\n",
    "                                 datetime.today().strftime('%Y_%m_%d_%H_%M_%S')+'_stop_word.txt')\n",
    "            with open(fname, 'w') as f:\n",
    "                freq_dct = dict(self.word_df['word'].value_counts())\n",
    "                text = ''\n",
    "                for w in sorted(freq_dct, key=freq_dct.get, reverse=True):\n",
    "                    text += '{} {}\\n'.format(freq_dct[w], w)\n",
    "                f.write(text)\n",
    "                log('Saved {} words to {}'.format(len(freq_dct), fname))\n",
    "\n",
    "        X['clean_text'] = X['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if w.lower() not in self.stop_words]).strip())\n",
    "        log('Stop words cleaned from texts')\n",
    "        start_len = X.shape[0]\n",
    "        # X = X.drop(X[X.clean_text.isin(self.stop_words)].index)\n",
    "        # X = X.drop(X[X.clean_text == ''].index)\n",
    "        X.loc[X.clean_text.isin(self.stop_words), 'clean_text'] = ''\n",
    "        log('Deleted {} texts containing stop-words only'.format(start_len - X.shape[0]))\n",
    "        log_size(X)\n",
    "        #X.to_csv(_debug_csv, columns=['clean_text', 'text'])\n",
    "        return X\n",
    "\n",
    "class NiceStemWords(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, folder=''):\n",
    "        self.data_folder = folder\n",
    "        self.nice_forms = {\n",
    "            'електронні': 'електронн',\n",
    "            'статте': 'статт',\n",
    "            'статті': 'статт',\n",
    "            'пропозиці': 'пропозиц',\n",
    "            'спрощено': 'спрощен',\n",
    "            'осіб': 'особ',\n",
    "            'тендерно': 'тендерн',\n",
    "            'україн': 'украін',\n",
    "            'уповноважено': 'уповноважен',\n",
    "            'договірно': 'договірн',\n",
    "            'договор': 'договір',\n",
    "            'фізично': 'фізичн',\n",
    "            'юридично': 'юридичн',\n",
    "            'днів': 'день',\n",
    "            'дня': 'день',\n",
    "            'можут': 'мож',\n",
    "            'вартост': 'вартіст',\n",
    "            'банківсько': 'банківськ',\n",
    "            'банківськи': 'банківськ',\n",
    "            'відміня': 'відмін',\n",
    "            }\n",
    "        self.counter = 0\n",
    "\n",
    "    def make_nice_word(self, word):\n",
    "        if word in self.nice_forms:\n",
    "            self.counter += 1\n",
    "            return self.nice_forms[word]\n",
    "        return word\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.counter = 0\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #log('--- NiceStemWords transform')\n",
    "        X['clean_text'] = X['clean_text'].apply(lambda x:' '.join([self.make_nice_word(w) for w in x.split()]))\n",
    "        log('{} words replaced with nice form'.format(self.counter))\n",
    "        return X\n",
    "\n",
    "class AttentionWord(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, folder=''):\n",
    "        self.data_folder = folder\n",
    "        self.attention_words = ['допуск', '123']\n",
    "        self.counter = 0\n",
    "\n",
    "    def mark_attention_word(self, word):\n",
    "        if word in self.attention_words:\n",
    "            self.counter += 1\n",
    "            return '_{}_'.format(word)\n",
    "        return word\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.counter = 0\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #log('--- AttentionWord transform')\n",
    "        X['clean_text'] = X['clean_text'].apply(lambda x:' '.join([self.mark_attention_word(w) for w in x.split()]))\n",
    "        log('{} attention words marked'.format(self.counter))\n",
    "        #X.to_csv(_debug_csv, columns=['clean_text', 'text'])\n",
    "        return X\n",
    "\n",
    "class CleanPunct(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #log('--- CleanPunct transform')\n",
    "        X['clean_text'] = X['clean_text'].apply(lambda x: preprocessing.strip_punctuation(x))\n",
    "        X['clean_text'] = X['clean_text'].apply(lambda x: preprocessing.strip_multiple_whitespaces(x).strip())\n",
    "        pattern = r'[«»]'\n",
    "        X['clean_text'] = X['clean_text'].apply(lambda x: re.sub(pattern=pattern, repl=\"\", string=x).strip())\n",
    "        #X.to_csv(_debug_csv, columns=['clean_text', 'text'])\n",
    "        # print_most_freq(X, 'clean_text', 500, msg='frequent texts')\n",
    "        return X\n",
    "\n",
    "class CleanNumberedLists(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #log('--- CleanNumberedLists transform')\n",
    "        pattern = r'^\\d+[(.\\d+){1,5}]*(\\)*|\\s+|\\\\.*)'\n",
    "        X['clean_text'] = X['clean_text'].apply(lambda x: re.sub(pattern=pattern, repl=\"\", string=x).strip())\n",
    "        # X.to_csv(_debug_csv, columns=['clean_text', 'text'])\n",
    "        X = drop_epmty(X, 'clean_text', 'after numbered lists cleaning')\n",
    "        log_size(X)\n",
    "        return X\n",
    "\n",
    "class NGram(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, trained_folder=''):\n",
    "        self.ngram = None\n",
    "        self.ngram_column = None\n",
    "        self.trained_folder = trained_folder\n",
    "        self.file_name = os.path.join(trained_folder, 'ngram.mod')\n",
    "\n",
    "    def generate_ngrams(self, src_df, n, min_count=10, threshold=25):\n",
    "        column_name = 'clean_text' if n==2 else str(n)+'gram_text'\n",
    "        prev_column_name = 'clean_text' if (n-1)==1 else str((n-1))+'gram_text'\n",
    "\n",
    "        gram_model = Phrases(word_src(src_df, prev_column_name), min_count, threshold)\n",
    "        gram_list = []\n",
    "        for txt in word_src(src_df, prev_column_name):\n",
    "            gram_txt = gram_model[txt]\n",
    "            for word in gram_txt:\n",
    "                if len(word.split('_'))==n:\n",
    "                    gram_list.append(word)\n",
    "        gram_df = pd.DataFrame(gram_list, columns=['word'])\n",
    "        src_df[str(n)+'gram_text'] = src_df.apply(lambda row: ' '.join(gram_model[row[prev_column_name].split()]), axis=1)\n",
    "\n",
    "        if not prev_column_name == 'clean_text':\n",
    "            del src_df[prev_column_name]\n",
    "\n",
    "        self.ngram = gram_model\n",
    "        self.ngram_column = column_name\n",
    "        return gram_df\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        gram_df = self.generate_ngrams(X, 2)\n",
    "\n",
    "        joblib.dump({\n",
    "            'ngram': self.ngram,\n",
    "        }, self.file_name)\n",
    "        log('Trained ngram model saved to {}'.format(self.file_name))\n",
    "\n",
    "        #print_most_freq(gram_df, 'word', 1000)\n",
    "        #gram_df.drop_duplicates(subset='word', inplace=True)\n",
    "        #gram_df.to_csv(_debug_csv, columns=['word'])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #log('--- NGram transform')\n",
    "        X['clean_text'] = X.apply(lambda row: ' '.join(self.ngram[row['clean_text'].split()]), axis=1)\n",
    "        #X.to_csv(_debug_csv, columns=['clean_text', 'text', 'hash'])\n",
    "        return X\n",
    "\n",
    "class StemWords(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stemmer = UkStemmer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #log('--- StemWords transform')\n",
    "        X['clean_text'] = X['clean_text'].apply(lambda x:' '.join([self.stemmer.stem_word(w) for w in x.split()]))\n",
    "        # X.to_csv(_debug_csv, columns=['clean_text', 'text'])\n",
    "        return X\n",
    "\n",
    "class TextRegistryTrain(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, trained_folder=''):\n",
    "        self.trained_folder = trained_folder\n",
    "        self.file_name = os.path.join(trained_folder, 'text_registry.csv')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        #log('--- TextUIDTrain fit')\n",
    "        hasher = murmur3_x64_128()\n",
    "        X['uid'] = X.apply(lambda row: hash_str(hasher, row['clean_text']), axis=1)\n",
    "        columns = ['uid', 'label_id', 'label_name', 'text', 'clean_text']\n",
    "        df = X[columns].copy()\n",
    "        df.drop_duplicates(subset=['uid'], inplace=True)\n",
    "        log('{} text UIDs saved to {}'.format(df.shape[0], self.file_name))\n",
    "        df.to_csv(self.file_name, columns=columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #log('--- TextUIDTrain transform')\n",
    "        return X\n",
    "\n",
    "class Doc2vecTrain(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, trained_folder=''):\n",
    "        self.doc2vec = None\n",
    "        self.trained_folder = trained_folder\n",
    "        self.file_name = os.path.join(trained_folder, 'vec_base.mod')\n",
    "\n",
    "    def tagged_doc_src(self, src_df, column):\n",
    "        for index, row in src_df.iterrows():\n",
    "            word_lst = str(row[column]).split()\n",
    "            if word_lst:\n",
    "                yield gensim.models.doc2vec.TaggedDocument(word_lst, [str(row['uid'])])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        #log('--- Doc2vecTrain fit')\n",
    "        train_corp = list(self.tagged_doc_src(X, column='clean_text'))\n",
    "        log('Prepared train corpus for Doc2vec, {} texts'.format(len(train_corp)))\n",
    "        #pprint(train_corp[:20])\n",
    "        #[6.1, 300, 200, 2] [7.4, 50, 100, 2] [6.1, 250, 100, 2] [7.7, 100, 100, 2]\n",
    "        vector_size, epochs, window = 300, 200, 2\n",
    "        log('Creating doc2vec model with hyperparameters: vector_sizes {} epochs {} window {} ...'.format(vector_size, epochs, window))\n",
    "        self.doc2vec = gensim.models.doc2vec.Doc2Vec(documents=train_corp, vector_size=vector_size, window=window, min_count=1, workers=8, epochs=epochs)\n",
    "        log('Doc2vec model is ready, vocabulary {}'.format(len(self.doc2vec.wv.vocab)))\n",
    "        #pprint(self.vectorizer.wv.vocab)\n",
    "        joblib.dump({\n",
    "            'doc2vec': self.doc2vec,\n",
    "        }, self.file_name)\n",
    "        log('Trained Doc2vec model saved to {}'.format(self.file_name))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #log('--- Doc2vecTrain transform')\n",
    "        X.to_csv(_debug_csv, columns=['clean_text', 'text', 'uid'])\n",
    "        return X\n",
    "\n",
    "class NGramDB(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, trained_folder=''):\n",
    "        self.ngram = None\n",
    "        self.trained_folder = trained_folder\n",
    "        self.file_name = os.path.join(trained_folder, 'ngram.mod')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        #log('--- NgramDB fit')\n",
    "        model_dct = joblib.load(self.file_name)\n",
    "        self.ngram = model_dct['ngram']\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #log('--- NgramDB transform')\n",
    "        X['clean_text'] = X.apply(lambda row: ' '.join(self.ngram[row['clean_text'].split()]), axis=1)\n",
    "        #X.to_csv(_debug_csv, columns=['clean_text', 'text'])\n",
    "        return X\n",
    "\n",
    "class TextRegistryDB(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, trained_folder=''):\n",
    "        self.reg_df = None\n",
    "        self.trained_folder = trained_folder\n",
    "        self.file_name = os.path.join(trained_folder, 'text_registry.csv')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # log('--- TextRegistryDB fit')\n",
    "        self.reg_df = pd.read_csv(self.file_name, usecols=['uid','label_id', 'label_name', 'text', 'clean_text'])\n",
    "        self.reg_df.label_id = self.reg_df.label_id.astype(int)\n",
    "        #log(self.reg_df.head())\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # log('--- TextRegistryDB transform')\n",
    "        X['uid'] = None\n",
    "        hasher = murmur3_x64_128()\n",
    "        for i, row in X.iterrows():\n",
    "            X.loc[i, 'uid'] = hash_str(hasher, row['clean_text'])\n",
    "            # row['uid'] = hash_str(hasher, row['clean_text'])\n",
    "            # reg_rows = self.reg_df.loc[self.reg_df.uid == row['uid']]\n",
    "            # if not reg_rows.empty:\n",
    "            #     row['label_id'] = reg_rows['label_id'].values.tolist()[0]\n",
    "            #     row['label_name'] = reg_rows['label_name'].values.tolist()[0]\n",
    "        return X\n",
    "\n",
    "class DeepSimProd(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, trained_folder='', sim_coef=3, stack_size=3, epochs=1000):\n",
    "        self.trained_folder = trained_folder\n",
    "        self.file_name = os.path.join(trained_folder, 'deep_sim.mod')\n",
    "        self.doc2vec_file_name = os.path.join(trained_folder, 'vec_base.mod')\n",
    "\n",
    "        self.text_registry = None\n",
    "        self.doc2vec_db = None\n",
    "\n",
    "        self.stack_size = stack_size\n",
    "        self.sim_coef = sim_coef\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.label_stat = None\n",
    "\n",
    "    def ivec(self, word_list, alpha=0.025):\n",
    "        if self.doc2vec_db:\n",
    "            return self.doc2vec_db.infer_vector(word_list, epochs=self.epochs)\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def most_similar(self, vec, topn=30):\n",
    "        similars = []\n",
    "        sims = self.doc2vec_db.docvecs.most_similar(positive=[vec], topn=topn)\n",
    "\n",
    "        reg_df = self.text_registry.reg_df\n",
    "\n",
    "        for doc_id, sim in sims:\n",
    "            found_rows = reg_df[reg_df.uid == doc_id]\n",
    "            similars.append({\n",
    "                'uid': doc_id,\n",
    "                'label_name': found_rows['label_name'],\n",
    "                'label_id': found_rows['label_id'],\n",
    "                'text': found_rows['text'],\n",
    "                'clean_text': found_rows['clean_text'],\n",
    "                'sim': sim,\n",
    "            })\n",
    "        return similars\n",
    "\n",
    "    def update_label_stat(self):\n",
    "        tdf = self.text_registry.reg_df\n",
    "        label_freq = {}\n",
    "        label_names = {}\n",
    "        label_norm_freq = {}\n",
    "        total_cnt = 0\n",
    "        max_count = 0\n",
    "        for i, row in tdf.groupby('label_id').nunique().iterrows():\n",
    "            cnt = row['uid']\n",
    "            label_freq[i] = cnt\n",
    "            total_cnt += cnt\n",
    "\n",
    "            if cnt > max_count:\n",
    "                max_count = cnt\n",
    "\n",
    "            label_names[i] = tdf.loc[tdf['label_id'] == i, 'label_name'].iloc[0]\n",
    "            # calc unique texts count instead\n",
    "\n",
    "        for label_id in label_freq:\n",
    "            label_norm_freq[label_id] = label_freq[label_id]/max_count\n",
    "\n",
    "        self.label_stat = {\n",
    "            'freq': label_freq,\n",
    "            'norm_freq': label_norm_freq,\n",
    "            'name': label_names,\n",
    "            'total_cnt': total_cnt,\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        #log('--- DeepSimProd fit')\n",
    "        self.text_registry = TextRegistryDB(trained_folder=self.trained_folder)\n",
    "        self.text_registry.fit(X, y)\n",
    "\n",
    "        model_dct = joblib.load(self.doc2vec_file_name)\n",
    "        self.doc2vec_db = model_dct['doc2vec']\n",
    "\n",
    "        self.update_label_stat()\n",
    "        #log(self.label_stat)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #log('--- DeepSimProd transform')\n",
    "\n",
    "        X['label_id'].fillna(inplace=True, value=_no_label_id)\n",
    "        X.label_id = X.label_id.astype(int)\n",
    "        X['label_name'].fillna(inplace=True, value=_no_label_name)\n",
    "\n",
    "        X = self.text_registry.transform(X)\n",
    "\n",
    "        for i, row in X.iterrows():\n",
    "            vec=self.ivec(str(row['clean_text']).split())\n",
    "            sims = self.most_similar(vec, topn=self.stack_size)\n",
    "            labels_cnt = {}\n",
    "            labels_sim = {}\n",
    "\n",
    "            for sim in sims:\n",
    "                try:\n",
    "                    label_id = sim['label_id'].iloc[0]\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                if label_id in labels_cnt:\n",
    "                    labels_cnt[label_id] += 1\n",
    "                    labels_sim[label_id] += math.pow(sim['sim'], self.sim_coef)\n",
    "                else:\n",
    "                    labels_cnt[label_id] = 1\n",
    "                    labels_sim[label_id] = math.pow(sim['sim'], self.sim_coef)\n",
    "\n",
    "            labels_score = {}\n",
    "\n",
    "            for label_id in labels_cnt:\n",
    "                # Fraction of particular label set\n",
    "                norm_freq = math.log(1+labels_cnt[label_id])/self.label_stat['freq'][label_id]\n",
    "                total_norm_freq = norm_freq\n",
    "\n",
    "                # Look at similarity\n",
    "                score = total_norm_freq*labels_sim[label_id]\n",
    "\n",
    "                labels_score[label_id] = score\n",
    "\n",
    "            winner_id = sorted(labels_score, key=labels_score.get, reverse=True)[0]\n",
    "\n",
    "            vals = {\n",
    "                'label_id': winner_id,\n",
    "                'label_name': self.label_stat['name'][winner_id],\n",
    "\n",
    "            }\n",
    "\n",
    "            sims_name = {}\n",
    "            for w in sorted(labels_score, key=labels_score.get, reverse=True):\n",
    "                sims_name[self.label_stat['name'][w]] = labels_score[w]\n",
    "            vals.update({\n",
    "                'sims': labels_score,\n",
    "                'sims_named': sims_name,\n",
    "                'sim': labels_score[winner_id]\n",
    "            })\n",
    "\n",
    "            X.loc[i, 'label_id'] = winner_id\n",
    "            X.loc[i, 'label_name'] = self.label_stat['name'][winner_id]\n",
    "\n",
    "            #pprint(vals['sims_named'])\n",
    "            df = pd.DataFrame.from_dict(data=vals['sims_named'], orient='index', columns=['sim'])\n",
    "            #pprint(df.head())\n",
    "            # print_sorted(df, 0,topn=100,ascending=True)\n",
    "            #pprint(vals)\n",
    "\n",
    "        # X.loc[X.label_id == _no_label_id, 'label_id'] = ''\n",
    "        # X.loc[X.label_id == _no_label_id, 'label_name'] = ''\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.transform(X)\n",
    "        y = X.iloc[0:X.shape[0]]['label_id'].values.tolist()\n",
    "        return y\n",
    "\n",
    "class ProcessNotLabeledData(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, text_column='clean_text'):\n",
    "        self.no_label_name = _no_label_name\n",
    "        self.no_label_id = _no_label_id\n",
    "        self.text_column = text_column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Sort labels by text's count. For each label's text check, if exist not labeled row with same text. Set label, if it is\n",
    "        \"\"\"\n",
    "        X['label_id'].fillna(inplace=True, value=self.no_label_id)\n",
    "        X.label_id = X.label_id.astype(int)\n",
    "        X['label_name'].fillna(inplace=True, value=self.no_label_name)\n",
    "\n",
    "        not_labeled_cnt = X.loc[X.label_id == self.no_label_id, 'text_id'].count()\n",
    "        log('{} labeled and {} not labeled texts in training set'.format(X.shape[0]-not_labeled_cnt, not_labeled_cnt))\n",
    "\n",
    "        df = X.groupby('label_id')['text_id'].nunique().sort_values(ascending=True).reset_index(name='count')\n",
    "        # print(df)\n",
    "\n",
    "        ids_lst = X['label_id'].unique().tolist()\n",
    "        names_lst = X['label_name'].unique().tolist()\n",
    "        lab_names = dict(zip(ids_lst, names_lst))\n",
    "\n",
    "        all_labels_mask = (df.label_id != self.no_label_id)\n",
    "        for label_id in df.loc[all_labels_mask, 'label_id'].tolist():\n",
    "            lab_texts = X[X.label_id == label_id][self.text_column].unique().tolist()\n",
    "            no_label_mask = (X.label_id == self.no_label_id) & (X[self.text_column].isin(lab_texts))\n",
    "            X.loc[no_label_mask, 'label_id'] = label_id\n",
    "            X.loc[no_label_mask, 'label_name'] = lab_names[label_id]\n",
    "\n",
    "        #print('--------------------------------')\n",
    "        #df = X.groupby('label_id')['text_id'].nunique().sort_values(ascending=True).reset_index(name='count')\n",
    "        #print(df)\n",
    "        not_labeled_cnt_after = X.loc[X.label_id == self.no_label_id, 'text_id'].count()\n",
    "        log('{} texts got labels'.format(not_labeled_cnt - not_labeled_cnt_after))\n",
    "        log('{} labeled and {} not labeled texts now '.format(\n",
    "            X.shape[0] - not_labeled_cnt_after, not_labeled_cnt_after))\n",
    "        return X\n",
    "\n",
    "class TrainingPipeline:\n",
    "\n",
    "    def __init__(self, data_folder=None, trained_folder=None, a=None):\n",
    "        self.info = _logger.info if a is None else a.info\n",
    "        self.data_folder = data_folder\n",
    "        self.trained_folder = trained_folder\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "    def fit(self, data_folder=None, trained_folder=None, a=None):\n",
    "\n",
    "        self.info = _logger.info if a is None else a.info\n",
    "        self.data_folder = self.data_folder if data_folder is None else data_folder\n",
    "        self.trained_folder = self.trained_folder if trained_folder is None else trained_folder\n",
    "\n",
    "        model_folder = self.trained_folder\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, input_file=None, output_file=None, trained_folder=None, a=None):\n",
    "\n",
    "        self.info = _logger.info if a is None else a.info\n",
    "        self.trained_folder = self.trained_folder if trained_folder is None else trained_folder\n",
    "\n",
    "def run():\n",
    "    pass\n",
    "    # df = folder_csv_to_dataframe(_data_folder)\n",
    "    #\n",
    "    # train_pipe = Pipeline([\n",
    "    #     ('preprocess', PreprocessDF()),\n",
    "    #     ('clean_text', CleanText()),\n",
    "    #     ('stop_text', StopText(folder=_model_folder)),\n",
    "    #     ('clean_punct', CleanPunct()),\n",
    "    #     ('stop_word', StopWordTrain(folder=_model_folder)),\n",
    "    #     ('clean_numbered_lists', CleanNumberedLists()),\n",
    "    #     ('attention_words', AttentionWord(folder=_model_folder)),\n",
    "    #     ('stemmer', StemWords()),\n",
    "    #     ('nice_stemmer', NiceStemWords(folder=_model_folder)),\n",
    "    #     ('ngram', NGram(folder=_model_folder)),\n",
    "    #     ('text_registry_train', TextRegistryTrain(folder=_model_folder)),\n",
    "    #     ('doc2vec_train', Doc2vecTrain(folder=_model_folder)),\n",
    "    #     ])\n",
    "    # #df=train_pipe.fit_transform(df)\n",
    "    #\n",
    "    # text_registry_db = TextRegistryDB(folder=_model_folder)\n",
    "    # doc2vec_db = Doc2VecDB(folder=_model_folder)\n",
    "    # deep_sim = DeepSimProd(folder=_model_folder, text_registry=text_registry_db, doc2vec_db=doc2vec_db)\n",
    "    #\n",
    "    # test_pipe = Pipeline([\n",
    "    #     #('preprocess', PreprocessDF()),\n",
    "    #     ('clean_text', CleanText()),\n",
    "    #     #('stop_text', StopText(folder=self.trained_folder)),\n",
    "    #     ('clean_punct', CleanPunct()),\n",
    "    #     ('stop_word', StopWord(folder=_model_folder)),\n",
    "    #     ('clean_numbered_lists', CleanNumberedLists()),\n",
    "    #     ('attention_words', AttentionWord(folder=_model_folder)),\n",
    "    #     ('stemmer', StemWords()),\n",
    "    #     ('nice_stemmer', NiceStemWords(folder=_model_folder)),\n",
    "    #     ('ngram_database', NGramDB(folder=_model_folder)),\n",
    "    #     ('text_registry_database', text_registry_db),\n",
    "    #     ('doc2vec_database', doc2vec_db),\n",
    "    #     ('deep_sim', deep_sim),\n",
    "    #     ])\n",
    "    #\n",
    "    # texts = [\n",
    "    #     '- документ підтвердження статусу платника ПДВ або єдиного податку;',\n",
    "    #     '- перелік та обсяг робіт (окремий файл) без зазначення вартост.окремих складових робот, з підтвердженням, що Учасник виконає їх у повному обсязі, за підписом керівника або уповноваженої особи Учасника - юридичної особи, фізичної особи – підприємця, завірені печаткою (при наявності).',\n",
    "    #     'календарн_графік виконанн робіт відповідн наказ_міністерств регіональн розвитк будівництв украін 13 01 №2',\n",
    "    #     '00 коп.).',\n",
    "    #     'документами, що підтверджують повноваження посадової особи або представника учасника процедури закупівлі щодо підпису документів тендерної пропозиції',\n",
    "    #     'Забезпечення тендерної пропозиції повертається у разі:',\n",
    "    #     'Лист-згоду на обробку, використання, поширення та доступ до персональних даних.',\n",
    "    #     'витяг з Єдиного державного реєстру юридичних осіб та фізичних осіб – підприємців (або виписка)',\n",
    "    #\n",
    "    # ]\n",
    "    # df = pd.DataFrame(texts, columns=['text'])\n",
    "    # df=test_pipe.fit_transform(df)\n",
    "    # pprint(df.head(100))\n",
    "\n",
    "def train():\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    train_pipe = Pipeline([\n",
    "        ('preprocess', PreprocessDF()),\n",
    "        ('clean_text', CleanText()),\n",
    "        ('stop_text', StopText(folder=_model_folder)),\n",
    "        ('clean_punct', CleanPunct()),\n",
    "        ('stop_word', StopWordTrain(folder=_model_folder)),\n",
    "        ('clean_numbered_lists', CleanNumberedLists()),\n",
    "        ('attention_words', AttentionWord(folder=_model_folder)),\n",
    "        ('stemmer', StemWords()),\n",
    "        ('nice_stemmer', NiceStemWords(folder=_model_folder)),\n",
    "        ('ngram', NGram(trained_folder=_model_folder)),\n",
    "        ('process_not_labeled', ProcessNotLabeledData()),\n",
    "        ('text_registry_train', TextRegistryTrain(trained_folder=_model_folder)),\n",
    "        ('doc2vec_train', Doc2vecTrain(trained_folder=_model_folder)),\n",
    "        ])\n",
    "\n",
    "    X = folder_csv_to_dataframe(_data_folder)\n",
    "    X['label_id'].fillna(_no_label_id, inplace=True)\n",
    "    X['label_name'].fillna(_no_label_name, inplace=True)\n",
    "\n",
    "    transformer = ProcessNotLabeledData(text_column='text')\n",
    "    X = transformer.fit_transform(X)\n",
    "    y = X['label_id'].copy()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "    X_test.to_csv(_test_set_csv)\n",
    "    train_pipe.fit_transform(X_train)\n",
    "\n",
    "    log('---------------------------------------------------------------')\n",
    "    log('Model trained')\n",
    "    log('---------------------------------------------------------------')\n",
    "\n",
    "def cross_validate_deep_sim():\n",
    "\n",
    "    test_pipe = Pipeline([\n",
    "        #('preprocess', PreprocessDF()),\n",
    "        ('clean_text', CleanText()),\n",
    "        #('stop_text', StopText(folder=self.trained_folder)),\n",
    "        ('clean_punct', CleanPunct()),\n",
    "        ('stop_word', StopWord(trained_folder=_model_folder, save_frequency_to_csv=False)),\n",
    "        ('clean_numbered_lists', CleanNumberedLists()),\n",
    "        ('attention_words', AttentionWord(folder=_model_folder)),\n",
    "        ('stemmer', StemWords()),\n",
    "        ('nice_stemmer', NiceStemWords(folder=_model_folder)),\n",
    "        ('ngram_database', NGramDB(trained_folder=_model_folder)),\n",
    "        ('text_registry_database', TextRegistryDB(trained_folder=_model_folder)),\n",
    "        ('deep_sim', DeepSimProd(trained_folder=_model_folder, sim_coef=1, stack_size=1, epochs=1000)),\n",
    "        ])\n",
    "\n",
    "    X_test = pd.read_csv(_test_set_csv)\n",
    "    X_test['label_id'].fillna(_no_label_id, inplace=True)\n",
    "    X_test['label_name'].fillna(_no_label_name, inplace=True)\n",
    "\n",
    "    y_test = X_test['label_id']\n",
    "\n",
    "\n",
    "    sim_coef = [1, 3, 5]\n",
    "    stack_size = [1, 2, 3, 10, 20, 30, 50]\n",
    "\n",
    "\n",
    "    search = GridSearchCV(test_pipe, scoring=['f1_micro',],\n",
    "                          # cv=2,\n",
    "                          verbose = 10, refit='f1_micro', n_jobs=-1,\n",
    "                          param_grid={\n",
    "        'deep_sim__sim_coef': sim_coef,\n",
    "        'deep_sim__epochs': [500,],\n",
    "        'deep_sim__stack_size': stack_size,\n",
    "        },)\n",
    "    search.fit(X_test, y_test)\n",
    "\n",
    "    # print('--------------------------------------')\n",
    "    # print('cv_results:')\n",
    "    # print(search.cv_results_)\n",
    "    print('--------------------------------------')\n",
    "    print('best_params:')\n",
    "    print(search.best_params_)\n",
    "    print('--------------------------------------')\n",
    "    print('best_estimator:')\n",
    "    print(search.best_estimator_)\n",
    "    #print(search.score(X, y_test))\n",
    "\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    plt.style.use('dark_background')\n",
    "\n",
    "\n",
    "    scores = search.cv_results_['mean_test_f1_micro']\n",
    "    scores = np.array(scores).reshape(len(sim_coef), len(stack_size))\n",
    "\n",
    "    for ind, i in enumerate(sim_coef):\n",
    "        plt.plot(stack_size, scores[ind], label='sim_coef: ' + str(i))\n",
    "    plt.legend()\n",
    "    plt.xlabel('stack_size')\n",
    "    plt.ylabel('Mean score')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate():\n",
    "\n",
    "    test_pipe = Pipeline([\n",
    "        #('preprocess', PreprocessDF()),\n",
    "        ('clean_text', CleanText()),\n",
    "        #('stop_text', StopText(folder=self.trained_folder)),\n",
    "        ('clean_punct', CleanPunct()),\n",
    "        ('stop_word', StopWord(trained_folder=_model_folder, save_frequency_to_csv=False)),\n",
    "        ('clean_numbered_lists', CleanNumberedLists()),\n",
    "        ('attention_words', AttentionWord(folder=_model_folder)),\n",
    "        ('stemmer', StemWords()),\n",
    "        ('nice_stemmer', NiceStemWords(folder=_model_folder)),\n",
    "        ('ngram_database', NGramDB(trained_folder=_model_folder)),\n",
    "        ('text_registry_database', TextRegistryDB(trained_folder=_model_folder)),\n",
    "        ('deep_sim', DeepSimProd(trained_folder=_model_folder, sim_coef=1, stack_size=1, epochs=1000)),\n",
    "        ])\n",
    "\n",
    "\n",
    "    # text_registry_db = TextRegistryDB(trained_folder=_model_folder)\n",
    "    # text_registry_db.fit_transform(pd.DataFrame([], columns=['text']))\n",
    "    #\n",
    "    # reg_df = text_registry_db.reg_df\n",
    "    # size = min(10000, reg_df.shape[0])\n",
    "\n",
    "    #X = reg_df.iloc[0:size].copy()\n",
    "\n",
    "    X_test = pd.read_csv(_test_set_csv)\n",
    "    X_test['label_id'].fillna(_no_label_id, inplace=True)\n",
    "    X_test['label_name'].fillna(_no_label_name, inplace=True)\n",
    "\n",
    "    X = test_pipe.fit_transform(X_test)\n",
    "\n",
    "    X_pred = X_test.copy()\n",
    "    for i, row in X_pred.iterrows():\n",
    "        for j, _ in X.iterrows():\n",
    "            if X.loc[j, 'text_id'] == X_pred.loc[i, 'text_id']:\n",
    "                X_pred.loc[i, 'label_id'] = X.loc[i, 'label_id']\n",
    "                X_pred.loc[i, 'label_name'] = X.loc[i, 'label_name']\n",
    "                break\n",
    "\n",
    "    y_test = X_test['label_id']\n",
    "    y_pred = X_pred['label_id']\n",
    "\n",
    "    log('Accuracy {}'.format(accuracy_score(y_test, y_pred)))\n",
    "    report = str(classification_report(y_test, y_pred, zero_division=0,))\n",
    "    print('\\n{}'.format(report))\n",
    "\n",
    "\n",
    "cross_validate_deep_sim()\n",
    "#train()\n",
    "#evaluate()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-59b3698e",
   "language": "python",
   "display_name": "PyCharm (tender_cat)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}